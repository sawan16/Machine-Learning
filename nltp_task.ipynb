{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Part 01: Basic NLTP tasks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk as nl\n",
    "\n",
    "#nl.download()       # call it only first time to download necessary/all data related to NLTP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentanse tokenization\n",
    "\n",
    "example_text = \"The sky is pinkish-blue. You shouldn't eat cardboard.\"\n",
    "\n",
    "print(nl.tokenize.sent_tokenize(example_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word tokenization\n",
    "\n",
    "example = 'The weather is great, and Python is awesome.'\n",
    "\n",
    "tokenized_words = nl.tokenize.word_tokenize(example)\n",
    "\n",
    "print(tokenized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pos tagging\n",
    "\n",
    "tagged_words = nl.tag.pos_tag(tokenized_words)\n",
    "\n",
    "print(tagged_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS Meta-information\n",
    "\n",
    "nl.help.upenn_tagset('NN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chunking\n",
    "\n",
    "chunked_data = nl.chunk.ne_chunk(tagged_words)\n",
    "\n",
    "print(chunked_data)\n",
    "\n",
    "chunked_data.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# showing the parse tree of an already parsed sentance(wsj: wall street journal dataset)\n",
    "\n",
    "from nltk.corpus import treebank\n",
    "\n",
    "tree = treebank.parsed_sents('wsj_0001.mrg')[0] \n",
    "\n",
    "print(tree)\n",
    "\n",
    "tree.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining your own grammar\n",
    "\n",
    "grammar = nl.data.load('mygmr.cfg')\n",
    "\n",
    "grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "grammar written in mygmr.cfg\n",
    "\n",
    "S -> NP VP \n",
    "VP -> V NP| VP PP\n",
    "PP -> P NP\n",
    "NP -> DT N | DT N PP | 'I'\n",
    "DT -> 'a' | 'the'\n",
    "N -> 'man' | 'telescope'\n",
    "V -> 'saw'\n",
    "P -> 'with'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# parsing sentance using mygmr\n",
    "\n",
    "text44 = 'I saw the man with a telescope'\n",
    "\n",
    "t_word2 = nl.word_tokenize(text44)\n",
    "\n",
    "t_word2\n",
    "\n",
    "parser = nl.ChartParser(grammar)\n",
    "\n",
    "trees = parser.parse_all(t_word2)\n",
    "\n",
    "trees[0].draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removal of stop words\n",
    "\n",
    "\n",
    "stop_words = set(nl.corpus.stopwords.words('english'))\n",
    "\n",
    "word_tokens = nl.tokenize.word_tokenize(example)\n",
    "\n",
    "filtered_words = []\n",
    "\n",
    "for w in word_tokens:\n",
    "    if w not in stop_words:\n",
    "        filtered_words.append(w)\n",
    "\n",
    "print(stop_words)\n",
    "print(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#punctuation mark removal\n",
    "p = [',','?','.']\n",
    "word_tokens = filtered_words\n",
    "\n",
    "\n",
    "filtered2 = []\n",
    "\n",
    "for w in word_tokens:\n",
    "    if w not in p:\n",
    "        filtered2.append(w)\n",
    "\n",
    "print(word_tokens)\n",
    "print(filtered2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemming\n",
    "\n",
    "ps = nl.stem.PorterStemmer()\n",
    "set1 =[ \"argue\", \"argued\", \"argues\", \"arguing\" ]\n",
    "\n",
    "for w in set1:\n",
    "    print(ps.stem(w))\n",
    "    \n",
    "set2 =[ \"python\",\"pythoner\",\"pythoning\",\"pythoned\"]\n",
    "\n",
    "print('-------------------')\n",
    "for w in set2:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmetization\n",
    "\n",
    "# udhr -> universal declaration of human rights corpus\n",
    "\n",
    "udhr = nl.corpus.udhr.words('English-Latin1')\n",
    "\n",
    "udhr[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemming effect\n",
    "\n",
    "[ps.stem(wd) for wd in udhr[:20]]       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization effect\n",
    "\n",
    "wordnetlem = nl.WordNetLemmatizer()\n",
    "\n",
    "[wordnetlem.lemmatize(wd) for wd in udhr[:20]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Part 02:\n",
    "# Example problem: Sentiment Analysis from movie reviews\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#data set loading\n",
    "\n",
    "positive = open('rt-polarity-pos.txt','rb')\n",
    "\n",
    "negative = open('rt-polarity-neg.txt','rb')\n",
    "\n",
    "# showing five negative reviews from the start\n",
    "\n",
    "i=0\n",
    "while i<5 :\n",
    "    print(negative.readline())\n",
    "    i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_words =[]   # for frequancy demostration \n",
    "n_words =[]   # for frequancy demostration \n",
    "\n",
    "#preprocessing\n",
    "def remove_stop_words(w_token,flag):\n",
    "    stop_words = set(nl.corpus.stopwords.words('english'))\n",
    "    filtered_words = []\n",
    "    ps = nl.stem.PorterStemmer()\n",
    "    for tmp_word in w_token:\n",
    "        if tmp_word not in stop_words:\n",
    "            filtered_words.append(tmp_word)\n",
    "            if flag ==1:\n",
    "                p_words.append(tmp_word)\n",
    "            else:\n",
    "                n_words.append(tmp_word)\n",
    "        \n",
    "    return filtered_words\n",
    "   \n",
    "# flag is used to store positive and negative tokens separately  for frequancy demonstration \n",
    "# flag has default value = 2 (for switching off flag at test time)\n",
    "\n",
    "def process_sentence(s,flag=2):\n",
    "    \n",
    "    w_token = nl.tokenize.word_tokenize(str(s)) \n",
    "\n",
    "    punctuations = [''\"''\",'&',',','?','.',']','[','}','{','(',')','!','?',':',';','\"','\\'','\\\\n','\"\"']\n",
    "    t2 = []\n",
    "    for w in w_token:\n",
    "        if w not in punctuations:\n",
    "            t2.append(w)\n",
    "    t3 = remove_stop_words(t2,flag)\n",
    "    return {word: 1 for word in t3}\n",
    "    \n",
    "\n",
    "positive_data_array = []\n",
    "#with positive as ps:\n",
    "for p_review in positive:\n",
    "    positive_data_array.append([process_sentence(p_review.lower(),1),'pos'])\n",
    "        \n",
    "\n",
    "\n",
    "negative_data_array = []\n",
    "i= 0\n",
    "\n",
    "for n_review in negative:\n",
    "        processed = [process_sentence(n_review.lower(),0),'neg']   \n",
    "        negative_data_array.append(processed)\n",
    "        if(i<5):\n",
    "            print('review before processing->')\n",
    "            print(n_review)\n",
    "            print('review after processing->')\n",
    "            print(processed)\n",
    "            print('-------------------------')\n",
    "            i+=1\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# printing positive word tokens\n",
    "\n",
    "print(p_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# printing negative word tokens\n",
    "\n",
    "print(n_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequency distribution for positive words\n",
    "\n",
    "dist1 = nl.probability.FreqDist(p_words)\n",
    "dist1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding top 30 frequent positive words\n",
    "\n",
    "vocab_p = dist1.keys()\n",
    "\n",
    "freqwords1 = [w for w in vocab_p if len(w) > 5 and dist1[w] > 50]\n",
    "\n",
    "freqwords1[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequancy distribution for negative words\n",
    "\n",
    "dist2 = nl.probability.FreqDist(n_words)\n",
    "dist2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding top 30 frequent negative words\n",
    "\n",
    "vocab_n = dist2.keys()\n",
    "\n",
    "freqwords2 = [w for w in vocab_n if len(w) > 5 and dist2[w] > 50]\n",
    "\n",
    "freqwords2[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#partition into training and test set\n",
    "\n",
    "training_set = positive_data_array[:3000]+negative_data_array[:3000]\n",
    "test_set =positive_data_array[3000:]+negative_data_array[3000:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build classifier and test\n",
    "\n",
    "classifier = nl.NaiveBayesClassifier.train(training_set)\n",
    "print(nl.classify.util.accuracy(classifier,test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifiy new test instance\n",
    "\n",
    "print(classifier.classify(process_sentence('this is great movie. mindblowing aminations')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classifier.classify(process_sentence('too much boaring movie. waste of money')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
